{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6wlfVYgjfRfkLeywOOSvI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palbha/llm_rag/blob/main/1_simple_rag_freeofcost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple RAG: Enhancing Language Models with External Knowledge\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a powerful technique that combines the strengths of information retrieval and generative models. By incorporating external knowledge sources, RAG significantly improves the accuracy and factual grounding of language models.\n",
        "\n",
        "**Here's how Simple RAG works:**\n",
        "\n",
        "1. **Data Ingestion:**  The process begins by loading and preprocessing the text data that will serve as the external knowledge base.\n",
        "2. **Chunking:**  The data is then divided into smaller, manageable chunks to optimize retrieval efficiency.\n",
        "3. **Embedding Creation:** Each text chunk is transformed into a numerical representation called an embedding using a pre-trained embedding model. These embeddings capture the semantic meaning of the text.\n",
        "4. **Semantic Search:** When a user poses a query, a semantic search is performed using the query's embedding to identify the most relevant text chunks from the knowledge base.\n",
        "5. **Response Generation:** Finally, a language model utilizes the retrieved chunks as context to generate a comprehensive and informative response to the user's query.\n",
        "\n",
        "**In this notebook, we'll explore a basic implementation of Simple RAG, assess the quality of the generated responses, and discuss potential enhancements to further improve its performance.**\n",
        "\n",
        "The motivation & contents in the notebook are derived from \"https://github.com/FareedKhan-dev/all-rag-techniques/blob/main/1_simple_rag.ipynb\" - All thanks to FareedKhan"
      ],
      "metadata": {
        "id": "nMpHi56bwdaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install relevany libraries - The code can run on Google colab\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7Mhe0BVmTIV",
        "outputId": "e3c63e70-380b-430a-b275-dd5d0f8f471a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "def get_text(file_path):\n",
        "  with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "        return text\n",
        "def chunk_text(text, n, overlap):\n",
        "    \"\"\"\n",
        "    Chunks the given text into segments of n characters with overlap.\n",
        "\n",
        "    Args:\n",
        "    text (str): The text to be chunked.\n",
        "    n (int): The number of characters in each chunk.\n",
        "    overlap (int): The number of overlapping characters between chunks.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []  # Initialize an empty list to store the chunks\n",
        "\n",
        "    # Loop through the text with a step size of (n - overlap)\n",
        "    for i in range(0, len(text), n - overlap):\n",
        "        # Append a chunk of text from index i to i + n to the chunks list\n",
        "        chunks.append(text[i:i + n])\n",
        "\n",
        "    return chunks  # Return the list of text chunks\n",
        "\n",
        "\n",
        "\n",
        "def create_embeddings(text, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "    \"\"\"\n",
        "    Creates embeddings for the given text using a Hugging Face model.\n",
        "\n",
        "    Args:\n",
        "    text (str): The input text for which embeddings are to be created.\n",
        "    model_name (str): The model to be used for creating embeddings. Default is \"sentence-transformers/all-MiniLM-L6-v2\".\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary containing the embeddings.\n",
        "    \"\"\"\n",
        "    # Load the tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Generate the embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the embeddings (mean pooling of the last hidden state)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    return {\"embeddings\": embeddings}\n",
        "\n"
      ],
      "metadata": {
        "id": "5rEVWP6sqYai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/25_new.txt\"\n",
        "\n",
        "# Extract text from the PDF file\n",
        "extracted_text = get_text(pdf_path)\n",
        "\n",
        "# Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters\n",
        "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
        "\n",
        "# Print the number of text chunks created\n",
        "print(\"Number of text chunks:\", len(text_chunks))\n",
        "\n",
        "# Print the first text chunk\n",
        "print(\"\\nFirst text chunk:\")\n",
        "print(text_chunks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqfegauWqjGN",
        "outputId": "714aa441-bd44-4b76-a8e7-b6edfa500f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of text chunks: 7\n",
            "\n",
            "First text chunk:\n",
            "Pahalgam attack: Prime Minister Narendra Modi arrived in New Delhi on Wednesday morning after cutting his Saudi trip short due to the terror attack in Jammu and Kashmir's Pahalgam. He had a brief meeting with NSA Ajit Doval, external affairs minister S Jaishankar, and foreign secretary Vikram Misri at the airport.\n",
            "\n",
            "At least 26 tourists have been killed after unidentified gunmen opened fire on innocent civilians in Pahalgam's Baisaran in Jammu and Kashmir. Gunshots were heard in the area, following which security forces rushed there.\n",
            "\n",
            "Initial reports suggested a possible terror attack at a site frequented by tourists, the police said. Security forces have been rushed to the area, and an operation is currently underway, reported PTI.\n",
            "\n",
            "The incident occurred at around 3 pm when terrorists came down from the mountain in Baisaran valley and started firing at the tourists who frequent the place, which is often dubbed as 'mini Switzerland' because of its long, lush green meadows.\n",
            "\n",
            "Authorities \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = create_embeddings(text_chunks)"
      ],
      "metadata": {
        "id": "_kQeOKDArA7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Calculates the cosine similarity between two vectors.\n",
        "\n",
        "    Args:\n",
        "    vec1 (np.ndarray): The first vector.\n",
        "    vec2 (np.ndarray): The second vector.\n",
        "\n",
        "    Returns:\n",
        "    float: The cosine similarity between the two vectors.\n",
        "    \"\"\"\n",
        "    # Compute the dot product of the two vectors and divide by the product of their norms\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
      ],
      "metadata": {
        "id": "0bAlFsY-rCSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(query, text_chunks, embeddings, k=5):\n",
        "    \"\"\"\n",
        "    Performs semantic search on the text chunks using the given query and embeddings.\n",
        "\n",
        "    Args:\n",
        "    query (str): The query for the semantic search.\n",
        "    text_chunks (List[str]): A list of text chunks to search through.\n",
        "    embeddings (List[dict]): A list of embeddings for the text chunks.\n",
        "    k (int): The number of top relevant text chunks to return. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: A list of the top k most relevant text chunks based on the query.\n",
        "    \"\"\"\n",
        "    # Create an embedding for the query\n",
        "    query_embedding = create_embeddings(query)['embeddings']\n",
        "    similarity_scores = []  # Initialize a list to store similarity scores\n",
        "\n",
        "    # Calculate similarity scores between the query embedding and each text chunk embedding\n",
        "    for i, chunk_embedding in enumerate(embeddings):\n",
        "        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding))\n",
        "        similarity_scores.append((i, similarity_score))  # Append the index and similarity score\n",
        "\n",
        "    # Sort the similarity scores in descending order\n",
        "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    # Get the indices of the top k most similar text chunks\n",
        "    top_indices = [index for index, _ in similarity_scores[:k]]\n",
        "    # Return the top k most relevant text chunks\n",
        "    return [text_chunks[index] for index in top_indices]"
      ],
      "metadata": {
        "id": "3PdrHikRrHFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Tell me about Indus treaty\"\n",
        "\n",
        "# Perform semantic search to find the top 2 most relevant text chunks for the query\n",
        "top_chunks = semantic_search(query, text_chunks, response['embeddings'], k=2)\n",
        "\n",
        "# Print the query\n",
        "print(\"Query:\", query)\n",
        "\n",
        "# Print the top 2 most relevant text chunks\n",
        "for i, chunk in enumerate(top_chunks):\n",
        "    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5wr9eqerNym",
        "outputId": "e63e4849-da92-4e8a-fa55-019d90bda0b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Tell me about Indus treaty\n",
            "Context 1:\n",
            "isit to the US and Peru and will return to India at the earliest, according to an official statement released on Wednesday. Sitharaman had arrived in the US on Sunday for a six-day visit and was supposed to travel to Peru afterwards for a five-day trip.\n",
            "\n",
            "The Cabinet Committee on Security (CCS), chaired by PM Modi, has decided on five key measures in the aftermath of the attack. They are:\n",
            "\n",
            "(i) The Indus Waters Treaty of 1960 will be held in abeyance with immediate effect until Pakistan credibly and irrevocably abjures its support for cross-border terrorism.\n",
            "\n",
            "(ii) The Integrated Check Post Attari will be closed with immediate effect. Those who have crossed over with valid endorsements may return through that route before 01 May 2025.\n",
            "\n",
            "(iii) Pakistani nationals will not be permitted to travel to India under the SAARC Visa Exemption Scheme (SVES) visas. Any SVES visas issued in the past to Pakistani nationals are deemed cancelled. Any Pakistani national currently in India under an SVES vis\n",
            "=====================================\n",
            "Context 2:\n",
            " to India under the SAARC Visa Exemption Scheme (SVES) visas. Any SVES visas issued in the past to Pakistani nationals are deemed cancelled. Any Pakistani national currently in India under an SVES visa has 48 hours to leave India.\n",
            "\n",
            "(iv) The Defence/Military, Naval and Air Advisors in the Pakistani High Commission in New Delhi are declared Persona Non Grata. They have a week to leave India. India will be withdrawing its own Defence/Navy/Air Advisors from the Indian High Commission in Islamabad. These posts in the respective High Commissions are deemed annulled. Five support staff of the Service Advisors will also be withdrawn from both High Commissions.\n",
            "\n",
            "(v) The overall strength of the High Commissions will be brought down to 30 from the present 55 through further reductions, to be effected by 01 May 2025.\n",
            "=====================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "from google.colab import userdata\n",
        "huggingface_hub.login(token=userdata.get('HF_TOKEN'))"
      ],
      "metadata": {
        "id": "adUAImTXtbU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
        "\n",
        "# You can use Huggingface in case you don't want to use gemini - specify the model & give it a shot\n",
        "def generate_response(system_prompt, user_message, model_name=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
        "    \"\"\"\n",
        "    Generates a response from a Hugging Face model based on the system prompt and user message.\n",
        "\n",
        "    Args:\n",
        "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
        "    user_message (str): The user's message or query.\n",
        "    model_name (str): The model to be used for generating the response. Default is \"meta-llama/Llama-3.2-3B-Instruct\".\n",
        "\n",
        "    Returns:\n",
        "    str: The response from the AI model.\n",
        "    \"\"\"\n",
        "    # Load the tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    # Combine the system prompt and user message\n",
        "    input_text = f\"{system_prompt}\\n{user_message}\"\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Generate the response\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=500, num_return_sequences=1, temperature=0)\n",
        "\n",
        "    # Decode the response and return it\n",
        "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response_text\n",
        "\n"
      ],
      "metadata": {
        "id": "uvZ0oZrTsO1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "def generate_response(system_prompt, user_message):\n",
        "  client = genai.Client(api_key=userdata.get(\"gemini_api\"))\n",
        "\n",
        "  response = client.models.generate_content(\n",
        "      model=\"gemini-2.0-flash\",\n",
        "      contents=[system_prompt+user_message]\n",
        "  )\n",
        "  return (response.text)"
      ],
      "metadata": {
        "id": "pO-b4KT5u4pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the user prompt based on the top chunks\n",
        "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
        "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
        "\n",
        "# Generate AI response\n",
        "ai_response = generate_response(system_prompt, user_prompt)\n",
        "print(ai_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs4_ikV8tmhx",
        "outputId": "34e7fd52-abe2-4c38-a9b1-d1754a0d5bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Indus Waters Treaty of 1960 will be held in abeyance with immediate effect until Pakistan credibly and irrevocably abjures its support for cross-border terrorism.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}